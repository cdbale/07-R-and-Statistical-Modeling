---
title: "R and Statistical Modeling"
subtitle: "MCB Consulting Group (MKTG 411)"
output: 
  ioslides_presentation:
    widescreen: true
    css: style.css
---

## Marketing Analytics Process

<center>
![](Figures/process_model.png){width="900px"}
</center>

---

![](Figures/hex_analytics.png){width=500px}

## Inference

Remember that models *extract information* from the data to inform our managerial decision. While data wrangling and visualization can *suggest* patterns of interest, a model is often needed.

How we model the data depends on if we care about **inference** or only about *prediction*.

- Inference means "reaching a conclusion based on evidence."
- Inferential modeling may also be referred to as **statistical modeling** given its ties to statistics.
- We use inferential models to understand a process we don't observe.

Note that models used for inference can also be used to predict, but they are focused primarily on understanding unobserved processes.

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Consider | Consider the "Story"

Inference is needed for managerial decisions in marketing because we often want to **intervene** in a process we don't observe.

Let's look at an example...

## Consider | Consider the "Story"

Inference is needed for managerial decisions in marketing because we often want to **intervene** in a process we don't observe.

For example, suppose we are now consulting for Kroger, a large client in the grocery/CPG industry. The client is interested in the best way to allocate promotional spending in the soup category.

- What data would we use to inform this decision?
- What is a possible process we don't observe that produces sales of soup products?

Your story is the beginning of a model.

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Build | Build a Statistical Model

A model starts with a story, but we need to translate it into a mathematical equation. For example:

$$\text{sales} = 10 + 3 \times \text{promotion_spend}$$

Think of this equation as a *function*.

- What is the input?
- What is the output?
- What is the story?

---

We observe `sales` and `promotion_spend` in the data, but we don't observe the effect of `promotion_spend` on `sales`. 

The unobserved variables in a model are called **parameters** and are typically expressed using Greek letters:

$$\text{sales} = \beta_0 + \beta_1 \times \text{promotion_spend}$$

Here $\beta_0$ is the amount of `sales` when there is zero `promotion_spend` and $\beta_1$ is the effect of `promotion_spend` on `sales`.

Are we missing anything to this story?

---

To make this a *statistical* model we need to express that our model is not going to explain everything about our story:

$$\text{sales} = \beta_0 + \beta_1 \times \text{promotion_spend} + \epsilon$$

Here $\epsilon$ represents **statistical error**. Think of this $\epsilon$ as including every variable beyond `promotion_spend` that effects `sales`.

## All Models are Wrong (But Some Are Useful)

Every inferential model is a *simplification* of the *true* story (which we don't observe). But, models can still help us learn about parameters, assuming the model does a good job at telling the story.

<center>
![](Figures/models_rockets.png){width=700px}
</center>

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Preprocess | Preprocess Data for Modeling

When working with real data, we often need to *preprocess the data* in order to model it or to make it easier for the model to use. Think of this as **data wrangling for models**.

One common preprocessing problem is dealing with discrete data.

```{r echo=FALSE, message=FALSE}
# Demonstrate dummy coding.
library(tidyverse)

sub_category <- read_csv("soup_data.csv") |> 
  transmute(sub_category = Sub_Category)

one_dummy <- sub_category |> 
  filter(sub_category %in% c("CONDENSED SOUP", "RAMEN"))

one_dummy |> unique()
```

---

For the computer to understand the different *levels* of a discrete variable used in a model, we need to recode into *binary variables*.

```{r echo=FALSE, message=FALSE}
# One dummy.
one_dummy |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN'
  ) |> 
  select(sub_category, condensed_soup, ramen)
```

---

The more levels in a discrete variable, the more binary variables we need.

```{r echo=FALSE, message=FALSE}
# Many dummies.
sub_category |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN',
    dry_soup = '.data_DRY SOUP'
  ) |> 
  select(sub_category, condensed_soup, ramen, dry_soup)
```

---

One more thing: For statistical reasons, the model can't use *all* of the binary variables created from a single discrete variable. One of them needs to be dropped as a **reference level** or **baseline level**. If you had more than one discrete variable, *each* discrete variable would have its own baseline level.

All of this discrete variable preprocessing is called **dummy coding** (a.k.a., indicator coding).

```{r echo=FALSE, message=FALSE}
# Baseline level.
sub_category |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN',
    dry_soup = '.data_DRY SOUP'
  ) |> 
  select(sub_category, ramen, dry_soup)
```

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Fit | Fit the Model

When we **fit** the model (a.k.a., training, calibrating, or estimating the model) we are getting parameter estimates.

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Evaluate | Parameter Estimates, Significance, and Predictive Fit

Our goal is to use the model to *estimate* the parameters from the data. In other words, parameter estimates are the information we are extracting from the data to inform our managerial decision.

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Predict | Counterfactual Predictions

Once we have a best-fitting model, we want to predict what will happen if we intervene in the process in a certain way. This is called a **counterfactual**.

For example, once we have parameter estimates for $\beta_0$ and $\beta_1$ from fitting the model, we can play the counterfactual "what if" game.

Specifically, what would happen if we allocated certain amounts of `promotion_spend`? By combining the parameter estimates with possible budget allotments, we can predict `sales`.

## Science and Faith

> "If any man have ears to hear, let him hear." (Mark 4:23)

Using evidence to learn about what we don't (or *can't*) observe is tied closely to both the scientific process and faith.

1. We are interested in a story (or process).
2. We turn that story into a model and fit it to data.
3. We evaluate the evidence, conditioned on our model.
4. We use what we learn to refine our understanding of what we don't (or *can't*) observe, learning line upon line. 

How is this same process described in Moroni 10:3-5?

## Wrapping Up

*Summary*

- Discussed using models for inference, including an inferential workflow.
- Started with a story, and translated it into a statistical model.

*Next Time*

- Building linear models with parsnip.

*Supplementary Material*

- [Faith and Science: Symbiotic Pathways to Truth](https://speeches.byu.edu/talks/jamie-jensen/faith-science-symbiotic-pathways-truth/)

## Exercise 6

1. Do a check-in. What worked well for you in this past unit? What didn't go well? If you have any questions or need support, reach out to a TA or the instructor (you are not graded on this part!)
2. Have you stayed organized? If not, go back and organize the material from Unit 1. Recommit yourself to keeping files organized on your computer for Unit 2. (again - not graded on this, it's for your benefit!)
3. Read 'Case-02-Nielsen-Sales.pdf' document that is posted on Canvas (it is located in the same subsection as this lecture). Write a response about how you might analyze the data to address the client's needs (no more than a page). Submit your response as a Word document on Canvas.

